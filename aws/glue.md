# üîß AWS Glue ‚Äî ETL Serverless

> Servi√ßo totalmente gerenciado para extra√ß√£o, transforma√ß√£o e carga (ETL) de dados na AWS

---

## üìå Resumo

**AWS Glue** √© um servi√ßo totalmente gerenciado e serverless para ETL, que facilita:

- ‚úÖ Categorizar dados
- ‚úÖ Limpar e enriquecer datasets
- ‚úÖ Criar pipelines ETL escal√°veis
- ‚úÖ Mover dados entre fontes e destinos com confiabilidade
- ‚úÖ Construir um cat√°logo unificado de metadados (Data Catalog)

Ele fornece uma UI intuitiva semelhante a ferramentas ETL tradicionais, mas com todas as vantagens de uma arquitetura serverless: sem servidores, baixo custo, escalabilidade autom√°tica e profunda integra√ß√£o com os servi√ßos AWS.

---

## üéØ O que √© ETL?

**ETL (Extract, Transform, Load)** √© o processo cl√°ssico utilizado para:

- ‚úÖ **Extrair** dados de v√°rias fontes (bancos, sistemas legados, SaaS, Salesforce, etc.)
- ‚úÖ **Transformar** esses dados (limpeza, padroniza√ß√£o, enriquecimento, convers√£o, conformidade)
- ‚úÖ **Carregar** os dados para um destino (banco, data warehouse, data lake)

Esse fluxo √© essencial para consolidar dados para BI, analytics e tomada de decis√£o.

---

## üåê O Contexto Moderno: Big Data & Cloud

Com o surgimento da nuvem e SaaS:

- ‚úÖ Mais fontes de dados
- ‚úÖ Maior volume de dados
- ‚úÖ Maior velocidade (streaming, near real-time)
- ‚úÖ Mais variedade (semiestruturado, logs, IoT, JSON, Parquet, etc.)

Isso exigiu ferramentas de integra√ß√£o de dados mais avan√ßadas, capazes de lidar com:

- ‚úÖ Escalabilidade
- ‚úÖ Flexibilidade
- ‚úÖ Custos menores
- ‚úÖ Conectividade nativa com servi√ßos cloud

Surgem ent√£o as solu√ß√µes **ETL serverless**, nas quais voc√™ escreve c√≥digo sem precisar gerenciar a infraestrutura que o executa.

---

## üîÑ ETL Tradicional vs ETL Serverless (AWS Glue)

A palestra destaca que o trabalho de ETL n√£o √© o valor final ‚Äî **o valor est√° na an√°lise dos dados depois de preparados**.

O serverless melhora o processo ao:

- ‚úÖ Remover a necessidade de provisionar servidores
- ‚úÖ Reduzir custo e manuten√ß√£o
- ‚úÖ Acelerar pipelines
- ‚úÖ Permitir maior uso de dados em tempo real
- ‚úÖ Facilitar integra√ß√µes com fontes/destinos na nuvem

### üîΩ Compara√ß√£o Simplificada

| Aspecto | ETL Tradicional | ETL Serverless (AWS Glue) |
|---------|----------------|---------------------------|
| **Infraestrutura** | Servidores on-premises gerenciados manualmente | Totalmente gerenciado pela AWS |
| **Escalabilidade** | Limitada; exige compra de hardware | Escala autom√°tica |
| **Custo** | Alto (licenciamento + hardware + manuten√ß√£o) | Paga apenas pelo que usa |
| **Conex√£o com fontes cloud** | Pode exigir conectores adicionais | Nativo |
| **Desenvolvimento** | Ferramentas complexas, scripts manuais | Auto-gera√ß√£o de c√≥digo, UI e Glue Studio |
| **Tempo para iniciar** | Demorado | Minutos |
| **Modelo** | ETL cl√°ssico | ETL e ELT modernos, com Glue + Athena/Redshift |

---

## üß© AWS Glue ‚Äî O ETL Serverless da AWS

AWS Glue simplifica o processo com:

- ‚úÖ **Interface visual (Glue Studio)** para montar pipelines ETL
- ‚úÖ **Auto-gera√ß√£o de c√≥digo serverless** (Spark / Python)
- ‚úÖ **Cat√°logo de dados (Glue Data Catalog)**
- ‚úÖ **Conex√£o nativa** com S3, Redshift, RDS, DynamoDB, Kinesis, etc.
- ‚úÖ **Execu√ß√£o totalmente gerenciada** ‚Äî sem servidores
- ‚úÖ **Transforma√ß√µes, crawlers e job scheduling**

**No modelo serverless:**

- Dados continuam sendo extra√≠dos, transformados e carregados
- Mas todo o processamento acontece na nuvem, sem gerenciamento de infraestrutura
- O Glue lida com escalabilidade, paralelismo e falhas

---

## üîÑ O Processo ETL Continua Igual

Mesmo usando Glue, a ess√™ncia permanece:

- ‚úÖ Perfilamento dos dados
- ‚úÖ Extra√ß√£o de dados relevantes
- ‚úÖ Tratamento de dados n√£o estruturados / semiestruturados
- ‚úÖ Transforma√ß√µes
- ‚úÖ Carregamento em destinos como Amazon Redshift ou S3 (data lake)
- ‚úÖ Prepara√ß√£o para consumo por ferramentas de BI

**O objetivo final segue o mesmo:**

Preparar dados com qualidade para permitir an√°lises melhores e decis√µes mais precisas.

---

## üß© Componentes Principais do AWS Glue

### Glue Data Catalog

- ‚úÖ Cat√°logo centralizado de metadados
- ‚úÖ Armazena esquemas, tabelas e parti√ß√µes
- ‚úÖ Usado por Athena, Redshift Spectrum e EMR
- ‚úÖ Integra√ß√£o com Hive Metastore

#### Principais fun√ß√µes do Data Catalog

O Data Catalog √© criado e populado quando voc√™ executa crawlers. Ele funciona como um:

- ‚úÖ **Reposit√≥rio persistente de metadados**
- ‚úÖ Cont√©m tabelas, schemas, vers√µes de schema, jobs, conex√µes
- ‚úÖ √â usado por Athena, Redshift Spectrum, Glue ETL Jobs e outros servi√ßos

**Principais fun√ß√µes:**

- ‚úÖ Armazenar defini√ß√µes de tabelas e schemas inferidos
- ‚úÖ Manter hist√≥rico de vers√µes de schema
- ‚úÖ Fornecer metadados para ETL jobs
- ‚úÖ Servir como cat√°logo centralizado para toda a plataforma de dados AWS

O Glue atualiza automaticamente metadados conforme:

- ‚úÖ Novas colunas aparecem
- ‚úÖ Schemas mudam
- ‚úÖ Parti√ß√µes s√£o criadas

### Glue Crawlers

- ‚úÖ Descobrem automaticamente esquemas de dados
- ‚úÖ Escaneiam fontes de dados (S3, RDS, DynamoDB, etc.)
- ‚úÖ Populam o Data Catalog automaticamente
- ‚úÖ Suportam particionamento autom√°tico

#### O Papel do Crawler ‚Äî Primeiro Passo no Glue

O **Glue Crawler** √© respons√°vel por:

- ‚úÖ Conectar-se a data stores (S3, JDBC, etc.)
- ‚úÖ Ler arquivos e descobrir automaticamente seu schema
- ‚úÖ Criar ou atualizar tabelas no Glue Data Catalog
- ‚úÖ Detectar mudan√ßas de schema (ex.: novas colunas)
- ‚úÖ Atualizar metadados para uso posterior em ETL jobs, Athena, Redshift Spectrum etc.

**Como o Crawler funciona:**

1. Analisa o conte√∫do da origem usando **classifiers**
2. Determina o formato e o schema
3. Gera tabelas de metadados
4. Armazena tudo no Glue Data Catalog

> üí° **Importante:** O crawler **N√ÉO descobre rela√ß√µes entre tabelas**, apenas estrutura e metadados.

**Formas de execu√ß√£o:**

- ‚úÖ **Agendada** (cron, hor√°rios fixos)
- ‚úÖ **On-demand** (manual)
- ‚úÖ **Baseada em eventos** (ex.: novo arquivo S3 via EventBridge)

#### Criando um Crawler ‚Äî Passo a Passo

Para criar um crawler no Glue:

1. **Nomeie o crawler**
2. **Escolha o data store** (S3, JDBC etc.)
3. **Informe o caminho** (com suporte a padr√µes como `s3://bucket/prefix/*.json`)
4. **Opcionalmente, adicione fontes adicionais**
5. **Defina o IAM Role** que permitir√° acesso aos dados e ao Data Catalog
6. **Configure o schedule** (on-demand, eventos, cron)
7. **Defina a sa√≠da** ‚Äî qual database armazenar√° as tabelas
8. **Ajuste op√ß√µes avan√ßadas**

### Glue ETL Jobs

- ‚úÖ Scripts Python ou Scala para transforma√ß√µes
- ‚úÖ Baseados em Apache Spark
- ‚úÖ Execu√ß√£o serverless gerenciada
- ‚úÖ Auto-scaling baseado na carga

#### Desenvolvimento

O Glue gera automaticamente c√≥digo em:

- ‚úÖ **Python (PySpark)**
- ‚úÖ **Scala (Spark)**

Esse c√≥digo corresponde √†s etapas de extract, transform e load definidas no wizard do Glue.

**Voc√™ pode:**

- ‚úÖ Editar o script gerado
- ‚úÖ Criar transforma√ß√µes customizadas
- ‚úÖ Usar development endpoints para testar e depurar via IDE local
- ‚úÖ Escrever leitores, writers e transforms personalizados

#### Agendamento e Execu√ß√£o

AWS Glue Jobs podem ser executados:

- ‚úÖ **On-demand** (manual)
- ‚úÖ **Agendados** (cron, intervalos, hor√°rios)
- ‚úÖ **Disparados por eventos** (S3 upload, EventBridge, SNS, etc.)

Tamb√©m √© poss√≠vel:

- ‚úÖ Rodar m√∫ltiplos jobs em paralelo
- ‚úÖ Criar depend√™ncias entre jobs
- ‚úÖ Construir pipelines ETL mais complexos

**Logs:**

Todos os logs e alertas s√£o enviados ao Amazon CloudWatch, permitindo:

- ‚úÖ Monitoramento
- ‚úÖ Alertas
- ‚úÖ Troubleshooting

#### Como criar um ETL Job ‚Äî Fluxo resumido

1. **Escolha a fonte de dados**
   - Deve existir como tabela no Data Catalog
   - Se necess√°rio, inclui a connection JDBC/S3/etc.

2. **Defina o destino dos dados**
   - Cat√°logo existente
   - Ou cria√ß√£o din√¢mica das tabelas-alvo

3. **Personalize par√¢metros do job**
   - Configura√ß√µes de execu√ß√£o
   - DPUs
   - Libraries
   - Arguments

4. **Ajuste o script gerado automaticamente**
   - Adicione transforms
   - Inclua m√∫ltiplas fontes ou destinos

5. **Escolha como o job ser√° disparado**
   - Manual
   - Agenda
   - Trigger de evento

No fim, o Glue gera o script PySpark/Scala pronto para execu√ß√£o e customiza√ß√£o.

### Glue Studio

- ‚úÖ Interface visual para criar jobs ETL
- ‚úÖ Gera√ß√£o autom√°tica de c√≥digo
- ‚úÖ Visualiza√ß√£o de transforma√ß√µes
- ‚úÖ Teste e debug integrados

### Glue Workflows

- ‚úÖ Orquestra√ß√£o de m√∫ltiplos jobs
- ‚úÖ Depend√™ncias entre jobs
- ‚úÖ Agendamento e triggers
- ‚úÖ Monitoramento centralizado

### Classifiers ‚Äî Como o Glue Identifica Formatos

Um **classifier** l√™ o conte√∫do da origem e retorna:

- ‚úÖ O formato do arquivo (por exemplo: JSON)
- ‚úÖ O schema inferido

**Tipos de classifiers suportados:**

- ‚úÖ **JSON**
- ‚úÖ **CSV**
  - Delimitadores suportados: v√≠rgula, pipe (|), tab, ponto e v√≠rgula
- ‚úÖ **Web logs**
- ‚úÖ **Classificadores de bancos de dados**
- ‚úÖ **Custom classifiers** (para formatos propriet√°rios ou l√≥gicas espec√≠ficas)

**Por que usar classificadores customizados?**

- ‚úÖ Quando o formato n√£o segue padr√µes conhecidos
- ‚úÖ Quando √© necess√°rio criar schemas mais precisos
- ‚úÖ Quando a tabela padr√£o gerada pelo Glue n√£o atende os requisitos anal√≠ticos

**Inclus√£o / Exclus√£o de padr√µes:**

Voc√™ pode:

- ‚úÖ Excluir arquivos `.csv`
- ‚úÖ Excluir pastas espec√≠ficas do S3
- ‚úÖ Usar express√µes regulares para controle fino

Isso evita crawlers lendo arquivos indevidos e aceleram seu ETL.

### Connections ‚Äî Acessando Data Stores

Uma **Connection** define as propriedades para conectar o Glue a bancos de dados via JDBC.

Ela √© usada por:

- ‚úÖ Crawlers
- ‚úÖ ETL Jobs
- ‚úÖ Visualiza√ß√µes do Glue Studio

**Bancos compat√≠veis via JDBC:**

- ‚úÖ **Amazon Redshift**
- ‚úÖ **Amazon RDS:**
  - PostgreSQL
  - MySQL
  - MariaDB
  - Oracle
  - SQL Server
  - Aurora (todas as variantes)

O tipo de conex√£o define como o Crawler ir√° inferir o schema e gerar metadados.

---

## üí° Quando Usar AWS Glue

### ‚úÖ √â Indicado Quando:

- üîÑ Voc√™ precisa fazer ETL de dados na AWS
- üìä Trabalha com data lakes no S3
- üóÑÔ∏è Precisa catalogar e descobrir esquemas automaticamente
- üí∞ Quer modelo de custo pay-per-use
- üöÄ Precisa de escalabilidade autom√°tica
- üîó Quer integrar m√∫ltiplas fontes de dados AWS

### ‚ö†Ô∏è Quando N√ÉO Usar:

Apesar dos benef√≠cios, h√° situa√ß√µes onde o Glue pode n√£o ser a melhor escolha:

- ‚ùå **Pipelines ETL j√° existentes e maduros on-premises**
  - Ferramentas tradicionais podem estar profundamente integradas √† opera√ß√£o.

- ‚ùå **Depend√™ncia forte em linguagens que o Glue n√£o suporta**
  - Glue suporta apenas Python e Scala ‚Äî empresas com forte investimento em Java podem sentir limita√ß√£o.

- ‚ùå **Conectividade limitada**
  - Glue ainda n√£o possui out-of-the-box connectors para certos sistemas (ex.: Salesforce, SAP), ao contr√°rio de ferramentas ETL mais antigas.

- ‚ùå **Esquemas que mudam com muita frequ√™ncia**
  - Mudan√ßas podem exigir edi√ß√£o manual de jobs e scripts.

- ‚ùå **Glue ainda √© um produto relativamente jovem**
  - Nem sempre atende todos os cen√°rios empresariais que ferramentas ETL tradicionais suportam.

- ‚ùå **Transforma√ß√µes muito simples** ‚Üí Considere scripts Lambda
- ‚ùå **Processamento em tempo real cr√≠tico** ‚Üí Considere Kinesis Analytics
- ‚ùå **Precisa de controle total sobre infraestrutura** ‚Üí Considere EMR

## üîê Permiss√µes (IAM)

Para acessar fontes e destinos, o Job precisa de:

- ‚úÖ **IAM Role** com permiss√µes de leitura/escrita nos data stores
- ‚úÖ Permiss√µes de uso do Data Catalog
- ‚úÖ Chaves de acesso, caso necess√°rio

**Boas pr√°ticas:**

- ‚úÖ Nunca usar o root user
- ‚úÖ Garantir governan√ßa para dados sens√≠veis
- ‚úÖ Conceder acesso m√≠nimo necess√°rio (least privilege)

---

## üéÅ Benef√≠cios do Glue em rela√ß√£o ao ETL tradicional

### üü¶ Serverless

- ‚úÖ Sem servidores para gerenciar
- ‚úÖ Custos apenas enquanto jobs e crawlers est√£o rodando

### üü¶ Crawlers autom√°ticos

- ‚úÖ Descobrem schemas
- ‚úÖ Detectam mudan√ßas
- ‚úÖ Atualizam tabelas automaticamente
- ‚úÖ Podem disparar jobs ETL

### üü¶ Auto-Gera√ß√£o de C√≥digo

- ‚úÖ Scripts PySpark/Scala criados automaticamente
- ‚úÖ F√°cil extens√£o com c√≥digo customizado

### üü¶ Integra√ß√£o nativa

- ‚úÖ S3, Athena, Redshift, RDS, DynamoDB, Lake Formation
- ‚úÖ Glue Studio para pipelines visuais
- ‚úÖ Development endpoints para integra√ß√£o com IDEs

---

## üîó Integra√ß√£o com Ecossistema AWS

O Glue integra-se profundamente com:

- ‚úÖ **S3** - Data lakes e armazenamento
- ‚úÖ **Athena** - Consultas SQL sobre dados catalogados
- ‚úÖ **Redshift** - Data warehouse
- ‚úÖ **RDS** - Bancos relacionais
- ‚úÖ **DynamoDB** - NoSQL
- ‚úÖ **Kinesis** - Streaming de dados
- ‚úÖ **EMR** - Big Data processing
- ‚úÖ **QuickSight** - Visualiza√ß√£o e BI

---

## üìä Exemplo de Fluxo ETL com Glue

```
1. Crawler descobre dados no S3
   ‚Üì
2. Popula Glue Data Catalog
   ‚Üì
3. Glue ETL Job transforma dados
   ‚Üì
4. Carrega dados transformados no destino
   ‚Üì
5. Athena/Redshift consulta dados preparados
   ‚Üì
6. QuickSight visualiza resultados
```

---

## üí∞ Pre√ßos do AWS Glue

### ‚úî Cobran√ßas

- ‚úÖ **Crawlers** ‚Üí por segundo
- ‚úÖ **Jobs ETL** ‚Üí por segundo
- ‚úÖ **Data Catalog** ‚Üí mensal por objetos/metadados
- ‚úÖ **Development endpoints** ‚Üí por hora (DPU)

### ‚úî Custos aproximados

- ‚úÖ **1 DPU = 4 vCPUs + 16 GB RAM**
- ‚úÖ Pre√ßo nos EUA (N. Virginia): ~ **$0.44 por DPU-hora**

### ‚úî Gr√°tis

- ‚úÖ Primeiros **1 milh√£o de objetos** no Data Catalog
- ‚úÖ Primeiros **1 milh√£o de acessos mensais** ao cat√°logo

> üí° **Dica**: Use particionamento e otimize jobs para reduzir tempo de execu√ß√£o e custos

---

## üîó Recursos Adicionais

- [Documenta√ß√£o Oficial AWS Glue](https://docs.aws.amazon.com/glue/)
- [AWS Glue - P√°gina do Produto](https://aws.amazon.com/glue/)
- [Glue Studio - Interface Visual](https://docs.aws.amazon.com/glue/latest/ug/glue-studio.html)
- [Best Practices do Glue](https://docs.aws.amazon.com/glue/latest/dg/best-practices.html)

---

## ‚úÖ Checklist de Aprendizado

- [ ] Entender o conceito de ETL (Extract, Transform, Load)
- [ ] Compreender diferen√ßas entre ETL tradicional e serverless
- [ ] Conhecer os componentes principais do Glue
- [ ] Entender Glue Data Catalog e sua import√¢ncia
- [ ] Saber usar Crawlers para descobrir esquemas
- [ ] Entender como Crawlers funcionam e suas formas de execu√ß√£o
- [ ] Saber criar um Crawler passo a passo
- [ ] Compreender Classifiers e quando usar custom classifiers
- [ ] Entender Connections para acesso a bancos via JDBC
- [ ] Criar jobs ETL usando Glue Studio
- [ ] Entender integra√ß√£o com outros servi√ßos AWS
- [ ] Saber otimizar jobs para reduzir custos
- [ ] Entender workflows e orquestra√ß√£o

---

## üè∑Ô∏è Tags

`#aws` `#glue` `#etl` `#data-integration` `#data-lake` `#big-data` `#serverless` `#spark` `#data-catalog` `#crawlers` `#classifiers` `#connections`

---

## ‚úÖ Resumo Final

O AWS Glue fornece um ambiente ETL totalmente gerenciado e serverless que inclui:

- ‚úÖ **Crawler** para descobrir schemas
- ‚úÖ **Data Catalog** persistente
- ‚úÖ **Jobs ETL** em PySpark/Scala
- ‚úÖ **Agendamentos, triggers e pipelines complexos**
- ‚úÖ **Monitoramento via CloudWatch**
- ‚úÖ **Conex√µes JDBC** e integra√ß√£o com todo o ecossistema AWS

Al√©m disso, o Glue reduz custos, simplifica pipelines e acelera a prepara√ß√£o de dados.

**Mas, como toda tecnologia, n√£o √© perfeito:**

- ‚ö†Ô∏è Suporta apenas duas linguagens (Python e Scala)
- ‚ö†Ô∏è Possui menos conectores que ferramentas ETL maduras
- ‚ö†Ô∏è Pode n√£o substituir pipelines tradicionais j√° consolidados

**O ecossistema completo inclui:**

- ‚úÖ **Crawlers** ‚Üí Descobrem e catalogam metadados
- ‚úÖ **Classifiers** ‚Üí Interpretam formatos e schemas
- ‚úÖ **Connections** ‚Üí Conectam o Glue a bancos via JDBC
- ‚úÖ **Data Catalog** ‚Üí Centraliza metadados
- ‚úÖ **Jobs ETL serverless** ‚Üí Movem e transformam dados em escala

Ele reduz drasticamente o esfor√ßo manual e permite focar no que realmente importa:

**Preparar dados de forma r√°pida, padronizada e integrada para an√°lise e tomada de decis√£o.**

---

**√öltima atualiza√ß√£o**: üìÖ [DD/MM/YYYY]

